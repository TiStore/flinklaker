## 项目简述
在海量乘客和司机的背景下，当乘客通过网约车平台开始叫车时，系统能够根据当前周围车辆调度情况，实时智能分配网约车到乘客。本项目将基于 TiDB + Flink + Pravega 的架构，提供在高密度、覆盖广的运力背景下，为网约车服务提供一个强大的实时智能调度系统，快速计算出哪个车、以什么路线能更快的接送乘客。

## 背景
 数字化时代，网约车服务的出现极大地优化了资源的配置，提高资源的利用率，提供海量工作机会的同时也为人类的出行提供了极大的便利。
    为了提高网约车服务的质量，更大的提升资源配置，从乘客和司机的角度出发，出现了以下基本需求：

#### 对乘客而言，最常见的期望如下：
- 能够在最短的时间内坐上车（叫车速度快，距离短）。
- 到达目的的时间最快（躲避拥堵&选最优路径）。
- 到达目的的成本最低（避开高速&跑的近）。
……

北京网约车需求动图：
![](https://img.huxiucdn.com/article/content/201905/04/183854999465.gif?imageView2/2/w/1000/format/gif/interlace/1/q/85)

#### 对司机而言，最基本的需求如下：
- 能够在成本最低的情况下接到乘客（接单距离相对行程距离足够短）。
- 尽量能避免空车回，如能够选择想去的目的地，如离自己家近一些。
- 能够预测到哪些地方的网约车需求较大（希望能多跑单）
……

北京网约车出行轨迹:
![](https://img.huxiucdn.com/article/content/201905/04/183854005737.gif?imageView2/2/w/1000/format/gif/interlace/1/q/85)

这些用户的基本需求，在网约车服务出现最初几年，数据量小，我们可以使用各种算法简单粗暴地达到目的。然而随着网约车车服务越来越广泛，司机与乘客数量的剧增，加上交通状况瞬息万变，最初的系统越跑越慢，往往数据刚跑出来，就已经失效了（比如刚给乘客算出最优司机时，该司机已经被其它人叫走了）。在海量数据背景下，对网约车调度系统的实时性要求也越来越高，这就让我们不得不考虑，如何能够设计出一套完美架构的解决方案，帮助我们提升网约车服务平台的质量。

## 目标

面对海量乘客和司机，瞬息万变的城市交通情况，平台能够**实时快速智能**地分配网约车，极大提升乘客与司机在叫车过程中的效率与体验。

## 实施方案
- 对于平台的订单交易系统，一般的后台会采用关系型数据库来存取订单，考虑到海量司机与乘客，我们采用分布式数据库 TiDB 作为订单交易系统的数据库。
- Flink 作为大数据流计算的新贵，近几年发展迅速且日渐稳重，因此我们使用 Flink 为本系统提供大数据计算服务。
- 直接从 TiDB 取订单数据到 Flink 计算，会给提供订单交易系统的 TiDB 过多的压力，所以我们考虑将 TiDB 的订单数据通过 TiCDC 实时同步到 Pravega, Flink 通过消费 Pravega 中的数据实时获取到当前所有车辆的状态。

综上几点考虑，我们将：
- 使用 TiDB 实时存储 车辆运营情况，乘客基本信息，当前订单状态。
- 使用 TiCDC 实时将乘客上下车事件所造成的数据变动，通过 Pravega 同步到 Flink 内存。
- 当收到新订单需求时，flink 实时计算并分配司机到对应乘客

## 成员介绍
- [Shirly](https://github.com/andremouche): 天池账号：AuntShirly。资深可乐炸鸡程序媛，分布式存储研发工程师，开源软件爱好者。
- [丘岳](https://github.com/CabinfeverB)： 天池账号：cabinfever。 资深分布式调度系统专家，开源软件爱好者。


## 参赛 issue 地址

https://github.com/flink-china/flink-forward-asia-hackathon-2021/issues/20